extend layout

block content
	.content-wrap.group
		section#design-content
			h1  Testing and Validation
			article.design.group
				.blockp
					h2 Testing the API using postman
					p
						| The majority of API call testing was done using Postman, a Google Chrome App with the functionality required for correctly testing a server, as it gave us the ability to formulae http requests to specific URLs. The advantage of this approach to testing meant that we were able to specify the type of http requests, be it POST, PUT, GET etc. In addition, the body of the request could be specified allowing for data to be posted to the database through the API calls in postman.
					p
						img(src='images/postman1.png', alt='Picture of postman example 1' width="700")
					p(align='center') Figure 1: Postman example of POST request for comment
					p
						| Due to careful planning and design considerations of the API, we had mapped out the use cases along with the URLs we expected those uses to use. The advantage of this is that in testing these became our test cases for example; a call to the API with a malformed URL like api/scores should correctly return a 404-response code. For our specific API there were a limited number of outcomes expected from each call. A post call for example would either result in 200 success indicating that data was added to the database successfully or 400 indicating that the data was unable to be uploaded properly.&nbsp; For an update to a score, a successful put would result in a success response code and a failure should be a 404 indicating that the required document could not be found in the database, and so no update was performed.
					p
						img(src='images/postman2.png', alt='Picture of postman example 2' width="700")
					p(align='center') Figure 2: Postman example highscore GET
					p
						| The API is structured such that it responds with an error code as well as a JSON object containing whatever data was collected from by the request. This is useful for checking the returned data from the database and ensuring that the model was working correctly. Using this testing environment and methodology was a crucial step in the application construction as it enabled us to test the API before implementing it completely into the website. When it was determined that the API was working correctly, implementation was a lot easier, as error checking meant we were better able to isolate issues to either forming the request before it is send to the API or dealing with the data that is returned by the API.

					h2 Deployment of the website
					p
						| In order to fully deploy this website, small changes had to be made to the locally run website code. Notably, previous references to the database were to the locally running test database in the mongo shell. To change this required changing these references to use the DBURI of an online mongodb such as the one supplied by mongolab, which is the chosen online database for this application.  After this the project was pushed to a Heroku project through git, allowing the project to be accessible and usable on the internet.
					p
						| Using mLab not only removed the load locally but allowed for more in-depth testing of each collection created and called by the heroku application. This meant that when users registered, posted a comment or played the game online, the user, comment and highscore collections could all be analysed and cross checked, ensuring that all the APIâ€™s were working correctly. An example of such testing can be seen below
					p
						img(src='images/mlabscollections.png', alt='Picture of mlabs collections screen' width="700")

					p(align='center') Figure 3: All collections from mlab mongo database
					p
						img(src='images/highscorescollection.png', alt='Picture of highscores collection' width="700")

					p(align='center') Figure 4: mLab display of highscore collection for all users

					p
						img(src='images/userscollection.png', alt='Picture of users collection' width="700")

					p(align='center') Figure 5: mLab display of users collection

					p
						| The final stage of testing was done using the Heroku log. This was used to located the origin of errors in the application logic and pin point exactly were these errors were encountered in the flow of information and routing. The problems encountered throughout the launch of the application, were quickly mitigated through these the use of this technique and again were cross check with results in mLab. Some sample screenshots of the heroku error messages can be seen below

					p
						img(src='images/herokulogserror.png', alt='Picture of heroku logs error for MVC picture get' width="700")
					p(align='center') Figure 6: Heroku error log for unsuccessful get leading to broken image in site. 
					p
						img(src='images/herokulogs2.png', alt='Picture of heroku logs error for MVC picture get' width="700")

					p(align='center') Figure 7: Log of successful get request for resources

					p
						| The final step, as with any good website was to validate the code, this was doing using #[a(href="https://jigsaw.w3.org/css-validator/") W3Schools CSS Validator]. The end result was a validation image seen below, that can also be found on the footer of every page and proves that "care was taken to create an introperable Web page".

					p
						a(href='http://jigsaw.w3.org/css-validator/check/referer')
						img(style='width:88px;height:31px', src='http://jigsaw.w3.org/css-validator/images/vcss-blue', alt='Valid CSS!')